{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f4b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:28: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\kriti\\AppData\\Local\\Temp\\ipykernel_20972\\3756699306.py:28: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  \"C:\\\\Users\\\\kriti\\\\OneDrive\\\\Desktop\\\\Infosys\\\\code\\Dataset\\\\UK\\\\SkilledWorker.pdf\",\n",
      "C:\\Users\\kriti\\AppData\\Local\\Temp\\ipykernel_20972\\3756699306.py:29: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  \"C:\\\\Users\\\\kriti\\\\OneDrive\\\\Desktop\\\\Infosys\\\\code\\Dataset\\\\UK\\\\HealthCare.pdf\",\n",
      "C:\\Users\\kriti\\AppData\\Local\\Temp\\ipykernel_20972\\3756699306.py:30: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  \"C:\\\\Users\\\\kriti\\\\OneDrive\\\\Desktop\\\\Infosys\\\\code\\Dataset\\\\UK\\\\Student.pdf\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXTRACTED] ‚Üí extracted\\Graduate.txt\n",
      "[EXTRACTED] ‚Üí extracted\\SkilledWorker.txt\n",
      "[EXTRACTED] ‚Üí extracted\\HealthCare.txt\n",
      "[EXTRACTED] ‚Üí extracted\\Student.txt\n",
      "[EXTRACTED] ‚Üí extracted\\Visitor.txt\n"
     ]
    }
   ],
   "source": [
    "#ATTEMPT 1\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "\n",
    "\n",
    "# ----------- PDF to TEXT Extraction ----------- #\n",
    "def extract_text(pdf_path, out_folder=\"extracted\"):\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "    text = \"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\\n\"\n",
    "\n",
    "    out_file = os.path.join(out_folder, Path(pdf_path).stem + \".txt\")\n",
    "\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f\"[EXTRACTED] ‚Üí {out_file}\")\n",
    "    return out_file\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdfs = [\n",
    "        \"C:\\\\Users\\\\kriti\\\\OneDrive\\\\Desktop\\\\Infosys\\\\code\\\\Dataset\\\\UK\\\\Graduate.pdf\",\n",
    "        \"C:\\\\Users\\\\kriti\\\\OneDrive\\\\Desktop\\\\Infosys\\\\code\\Dataset\\\\UK\\\\SkilledWorker.pdf\",\n",
    "        \"C:\\\\Users\\\\kriti\\\\OneDrive\\\\Desktop\\\\Infosys\\\\code\\Dataset\\\\UK\\\\HealthCare.pdf\",\n",
    "        \"C:\\\\Users\\\\kriti\\\\OneDrive\\\\Desktop\\\\Infosys\\\\code\\Dataset\\\\UK\\\\Student.pdf\",\n",
    "        \"C:\\\\Users\\\\kriti\\\\OneDrive\\\\Desktop\\\\Infosys\\\\code\\\\Dataset\\\\UK\\\\Visitor.pdf\"\n",
    "    ]\n",
    "\n",
    "    for pdf in pdfs:\n",
    "        extract_text(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ce144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Graduate ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\Graduate.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 144\u001b[0m\n\u001b[0;32m    135\u001b[0m docs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraduate\u001b[39m\u001b[38;5;124m\"\u001b[39m:      \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mkriti\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mInfosys\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mextracted\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGraduate.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkilledWorker\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mkriti\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mInfosys\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mextracted\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSkilledWorker.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisitor\u001b[39m\u001b[38;5;124m\"\u001b[39m:       \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mkriti\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mInfosys\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mextracted\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mVisitor.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m }\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m visa_type, path \u001b[38;5;129;01min\u001b[39;00m docs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 144\u001b[0m     \u001b[43mprocess_one_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisa_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ Semantic + size + overlap chunking completed for all docs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 103\u001b[0m, in \u001b[0;36mprocess_one_document\u001b[1;34m(visa_type, full_path)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö† No sentences found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m chunks_text \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_chunks_from_sentences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_WORDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverlap_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOVERLAP_WORDS\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Attach metadata\u001b[39;00m\n\u001b[0;32m    110\u001b[0m chunk_objs \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[2], line 49\u001b[0m, in \u001b[0;36mbuild_chunks_from_sentences\u001b[1;34m(sentences, max_words, overlap_words)\u001b[0m\n\u001b[0;32m     46\u001b[0m end \u001b[38;5;241m=\u001b[39m start\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Grow chunk until we hit max_words\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m end \u001b[38;5;241m<\u001b[39m n:\n\u001b[0;32m     50\u001b[0m     sent_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentences[end]\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word_count \u001b[38;5;241m+\u001b[39m sent_words \u001b[38;5;241m>\u001b[39m max_words:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ATTEMPT 2\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "\n",
    "# Where you want the new chunks to be saved\n",
    "OUTPUT_DIR = r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\"\n",
    "\n",
    "# Chunk configuration (you can tune these)\n",
    "MAX_WORDS = 250        # target max words per chunk\n",
    "OVERLAP_WORDS = 60     # how many words overlap between chunks\n",
    "\n",
    "\n",
    "# ================== UTILS ==================\n",
    "\n",
    "def read_text(path: str) -> str:\n",
    "    \"\"\"Read text file with UTF-8.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def simple_sentence_split(text: str):\n",
    "    \"\"\"\n",
    "    Rough sentence splitter based on punctuation.\n",
    "    Not perfect, but good enough for semantic-ish chunking.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    sentences = re.split(r'(?<=[\\.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_chunks_from_sentences(sentences, max_words=MAX_WORDS, overlap_words=OVERLAP_WORDS):\n",
    "    \"\"\"\n",
    "    Core semantic+size+overlap chunker.\n",
    "    We accumulate sentences until we reach max_words,\n",
    "    then create a chunk and start next one with overlap.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    n = len(sentences)\n",
    "    start = 0\n",
    "\n",
    "    while start < n:\n",
    "        word_count = 0\n",
    "        end = start\n",
    "\n",
    "        # Grow chunk until we hit max_words\n",
    "        while end < n:\n",
    "            sent_words = len(sentences[end].split())\n",
    "            if word_count + sent_words > max_words:\n",
    "                break\n",
    "            word_count += sent_words\n",
    "            end += 1\n",
    "\n",
    "        if end == start:\n",
    "            # Single very long sentence ‚Äì force include\n",
    "            end = start + 1\n",
    "\n",
    "        chunk_sentences = sentences[start:end]\n",
    "        chunk_text = \" \".join(chunk_sentences).strip()\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "        if end >= n:\n",
    "            break\n",
    "\n",
    "        # ---- Overlap calculation ----\n",
    "        overlap_count = 0\n",
    "        new_start = end\n",
    "\n",
    "        for i in range(end - 1, start - 1, -1):\n",
    "            overlap_count += len(sentences[i].split())\n",
    "            if overlap_count >= overlap_words:\n",
    "                new_start = i\n",
    "                break\n",
    "\n",
    "        start = new_start\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_one_document(visa_type: str, full_path: str):\n",
    "    \"\"\"\n",
    "    Read TXT directly from full_path, perform semantic+overlap chunking,\n",
    "    and save as JSON with metadata.\n",
    "    \"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    text_path = full_path  # we are using absolute path directly\n",
    "    if not os.path.exists(text_path):\n",
    "        print(f\"‚ö† File not found: {text_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcessing {visa_type} ‚Üí {text_path}\")\n",
    "\n",
    "    raw_text = read_text(text_path)\n",
    "    sentences = simple_sentence_split(raw_text)\n",
    "\n",
    "    if not sentences:\n",
    "        print(f\"‚ö† No sentences found in {text_path}\")\n",
    "        return\n",
    "\n",
    "    chunks_text = build_chunks_from_sentences(\n",
    "        sentences,\n",
    "        max_words=MAX_WORDS,\n",
    "        overlap_words=OVERLAP_WORDS\n",
    "    )\n",
    "\n",
    "    # Attach metadata\n",
    "    chunk_objs = []\n",
    "    for idx, ch in enumerate(chunks_text, start=1):\n",
    "        words_in_chunk = len(ch.split())\n",
    "        chunk_objs.append({\n",
    "            \"id\": f\"{visa_type}_{idx}\",\n",
    "            \"visa_type\": visa_type,\n",
    "            \"chunk_index\": idx,\n",
    "            \"source_file\": text_path,\n",
    "            \"word_count\": words_in_chunk,\n",
    "            \"max_words\": MAX_WORDS,\n",
    "            \"overlap_words\": OVERLAP_WORDS,\n",
    "            \"text\": ch\n",
    "        })\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"{visa_type}.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunk_objs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úî {visa_type}: {len(chunk_objs)} chunks saved ‚Üí {out_path}\")\n",
    "\n",
    "\n",
    "# ================== MAIN ==================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Map visa_type to FULL TXT paths (not PDFs)\n",
    "    docs = {\n",
    "        \"Graduate\":      r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\Graduate.txt\",\n",
    "        \"SkilledWorker\": r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\SkilledWorker.txt\",\n",
    "        \"HealthCare\":    r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\HealthCare.txt\",\n",
    "        \"Student\":       r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\Student.txt\",\n",
    "        \"Visitor\":       r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\Visitor.txt\"\n",
    "    }\n",
    "\n",
    "    for visa_type, path in docs.items():\n",
    "        process_one_document(visa_type, path)\n",
    "\n",
    "    print(\"\\n‚úÖ Semantic + size + overlap chunking completed for all docs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63740de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Graduate ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\Graduate.txt\n",
      "   ‚úÖ Total chunks built: 7\n",
      "‚úî Graduate: 7 chunks saved ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\\Graduate.json\n",
      "\n",
      "Processing SkilledWorker ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\SkilledWorker.txt\n",
      "   ‚úÖ Total chunks built: 7\n",
      "‚úî SkilledWorker: 7 chunks saved ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\\SkilledWorker.json\n",
      "\n",
      "Processing HealthCare ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\HealthCare.txt\n",
      "   ‚úÖ Total chunks built: 7\n",
      "‚úî HealthCare: 7 chunks saved ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\\HealthCare.json\n",
      "\n",
      "Processing Student ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\Student.txt\n",
      "   ‚úÖ Total chunks built: 6\n",
      "‚úî Student: 6 chunks saved ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\\Student.json\n",
      "\n",
      "Processing Visitor ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\Visitor.txt\n",
      "   ‚úÖ Total chunks built: 9\n",
      "‚úî Visitor: 9 chunks saved ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\\Visitor.json\n",
      "\n",
      "‚úÖ Semantic + size + overlap chunking completed for all docs.\n"
     ]
    }
   ],
   "source": [
    "#ATTEMPT 3\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "\n",
    "# Folder where your .txt files are stored\n",
    "EXTRACTED_DIR = r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\"\n",
    "\n",
    "# Folder where you want chunks to be saved\n",
    "OUTPUT_DIR = r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\"\n",
    "\n",
    "# Chunk configuration (tune if needed)\n",
    "MAX_WORDS = 250        # target max words per chunk\n",
    "OVERLAP_WORDS = 80     # approx words to overlap between chunks\n",
    "\n",
    "\n",
    "# ================== UTILS ==================\n",
    "\n",
    "def read_text(path: str) -> str:\n",
    "    \"\"\"Read text file with UTF-8.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Robust-ish sentence splitter:\n",
    "    - Normalize spaces\n",
    "    - Split on . ? !\n",
    "    - Further split very long segments by ; : and ,\n",
    "    This makes long legal paragraphs more manageable.\n",
    "    \"\"\"\n",
    "    # normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # rough sentence split on punctuation\n",
    "    rough = re.split(r\"(?<=[\\.!?])\\s+\", text)\n",
    "\n",
    "    sentences = []\n",
    "    for seg in rough:\n",
    "        seg = seg.strip()\n",
    "        if not seg:\n",
    "            continue\n",
    "\n",
    "        # if segment is very long, break on ; or :\n",
    "        if len(seg.split()) > 120:\n",
    "            parts = re.split(r\"(?<=[;:])\\s+\", seg)\n",
    "        else:\n",
    "            parts = [seg]\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if not part:\n",
    "                continue\n",
    "\n",
    "            # if still huge, break again by comma\n",
    "            if len(part.split()) > 120:\n",
    "                subparts = re.split(r\",\\s+\", part)\n",
    "                for sp in subparts:\n",
    "                    sp = sp.strip()\n",
    "                    if sp:\n",
    "                        sentences.append(sp)\n",
    "            else:\n",
    "                sentences.append(part)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_chunks_from_sentences(sentences, max_words=MAX_WORDS, overlap_words=OVERLAP_WORDS):\n",
    "    \"\"\"\n",
    "    Build chunks from a list of sentences:\n",
    "    - Each chunk up to max_words\n",
    "    - Overlap approx overlap_words\n",
    "    - GUARANTEED forward progress (no infinite loops)\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    n = len(sentences)\n",
    "    idx = 0\n",
    "    chunk_num = 0\n",
    "\n",
    "    while idx < n:\n",
    "        start_idx = idx\n",
    "        words = 0\n",
    "        end_idx = idx\n",
    "\n",
    "        # grow chunk until max_words reached\n",
    "        while end_idx < n:\n",
    "            sent_len = len(sentences[end_idx].split())\n",
    "            if words + sent_len > max_words and words > 0:\n",
    "                break\n",
    "            words += sent_len\n",
    "            end_idx += 1\n",
    "\n",
    "        # safety: ensure at least one sentence is included\n",
    "        if end_idx == start_idx:\n",
    "            end_idx = min(start_idx + 1, n)\n",
    "            words = len(sentences[start_idx].split())\n",
    "\n",
    "        # build the chunk text\n",
    "        chunk_text = \" \".join(sentences[start_idx:end_idx]).strip()\n",
    "        chunks.append(chunk_text)\n",
    "        chunk_num += 1\n",
    "\n",
    "        if chunk_num % 10 == 0:\n",
    "            print(f\"   ‚è≥ Chunks created so far: {chunk_num}\")\n",
    "\n",
    "        # if we've reached the end, stop\n",
    "        if end_idx >= n:\n",
    "            break\n",
    "\n",
    "        # ----- compute new start with overlap -----\n",
    "        overlap_count = 0\n",
    "        j = end_idx - 1\n",
    "\n",
    "        # walk backwards from end_idx-1 until we accumulate overlap_words\n",
    "        while j > start_idx and overlap_count < overlap_words:\n",
    "            overlap_count += len(sentences[j].split())\n",
    "            j -= 1\n",
    "\n",
    "        overlap_start_idx = j + 1  # first sentence to keep for overlap\n",
    "\n",
    "        # GUARANTEE forward movement\n",
    "        if overlap_start_idx <= start_idx:\n",
    "            idx = end_idx   # no useful overlap, just move on\n",
    "        else:\n",
    "            idx = overlap_start_idx\n",
    "\n",
    "    print(f\"   ‚úÖ Total chunks built: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_one_document(visa_type: str, filename: str):\n",
    "    \"\"\"\n",
    "    Read TXT from EXTRACTED_DIR, perform semantic chunking with overlap,\n",
    "    save JSON with metadata to OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    text_path = os.path.join(EXTRACTED_DIR, filename)\n",
    "    if not os.path.exists(text_path):\n",
    "        print(f\"‚ö† File not found for {visa_type}: {text_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcessing {visa_type} ‚Üí {text_path}\")\n",
    "\n",
    "    raw_text = read_text(text_path)\n",
    "    sentences = split_into_sentences(raw_text)\n",
    "\n",
    "    if not sentences:\n",
    "        print(f\"‚ö† No sentences found for {visa_type} in {text_path}\")\n",
    "        return\n",
    "\n",
    "    chunks_text = build_chunks_from_sentences(sentences)\n",
    "\n",
    "    # Attach metadata\n",
    "    chunk_objs = []\n",
    "    for idx, ch in enumerate(chunks_text, start=1):\n",
    "        words_in_chunk = len(ch.split())\n",
    "        chunk_objs.append({\n",
    "            \"id\": f\"{visa_type}_{idx}\",\n",
    "            \"visa_type\": visa_type,\n",
    "            \"chunk_index\": idx,\n",
    "            \"source_file\": filename,\n",
    "            \"word_count\": words_in_chunk,\n",
    "            \"max_words\": MAX_WORDS,\n",
    "            \"overlap_words\": OVERLAP_WORDS,\n",
    "            \"text\": ch\n",
    "        })\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"{visa_type}.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunk_objs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úî {visa_type}: {len(chunk_objs)} chunks saved ‚Üí {out_path}\")\n",
    "\n",
    "\n",
    "# ================== MAIN ==================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # visa_type ‚Üí corresponding TXT filename in EXTRACTED_DIR\n",
    "    docs = {\n",
    "        \"Graduate\":      \"Graduate.txt\",\n",
    "        \"SkilledWorker\": \"SkilledWorker.txt\",\n",
    "        \"HealthCare\":    \"HealthCare.txt\",\n",
    "        \"Student\":       \"Student.txt\",\n",
    "        \"Visitor\":       \"Visitor.txt\"\n",
    "    }\n",
    "\n",
    "    for visa_type, fname in docs.items():\n",
    "        process_one_document(visa_type, fname)\n",
    "\n",
    "    print(\"\\n‚úÖ Semantic + size + overlap chunking completed for all docs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc977bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Graduate ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\Graduate.txt\n",
      "‚úî Graduate: 7 chunks created.\n",
      "\n",
      "Processing SkilledWorker ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\SkilledWorker.txt\n",
      "‚úî SkilledWorker: 7 chunks created.\n",
      "\n",
      "Processing HealthCare ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\HealthCare.txt\n",
      "‚úî HealthCare: 7 chunks created.\n",
      "\n",
      "Processing Student ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\Student.txt\n",
      "‚úî Student: 6 chunks created.\n",
      "\n",
      "Processing Visitor ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\\Visitor.txt\n",
      "‚úî Visitor: 9 chunks created.\n",
      "\n",
      "üéâ ALL DONE! Unified chunk file saved ‚Üí C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\\all_visa_chunks.json\n",
      "Total chunks: 36\n"
     ]
    }
   ],
   "source": [
    "#ATTEMPT 4 ‚Üí UNIFIED CHUNK JSON\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "\n",
    "EXTRACTED_DIR = r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\extracted\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\"\n",
    "\n",
    "MAX_WORDS = 250\n",
    "OVERLAP_WORDS = 80\n",
    "\n",
    "\n",
    "# ================== UTILS ==================\n",
    "\n",
    "def read_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str):\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    rough = re.split(r\"(?<=[\\.!?])\\s+\", text)\n",
    "\n",
    "    sentences = []\n",
    "    for seg in rough:\n",
    "        seg = seg.strip()\n",
    "        if not seg:\n",
    "            continue\n",
    "\n",
    "        if len(seg.split()) > 120:\n",
    "            parts = re.split(r\"(?<=[;:])\\s+\", seg)\n",
    "        else:\n",
    "            parts = [seg]\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if not part:\n",
    "                continue\n",
    "\n",
    "            if len(part.split()) > 120:\n",
    "                subparts = re.split(r\",\\s+\", part)\n",
    "                sentences.extend([sp.strip() for sp in subparts if sp.strip()])\n",
    "            else:\n",
    "                sentences.append(part)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_chunks_from_sentences(sentences, max_words=MAX_WORDS, overlap_words=OVERLAP_WORDS):\n",
    "    chunks = []\n",
    "    n = len(sentences)\n",
    "    idx = 0\n",
    "    chunk_num = 0\n",
    "\n",
    "    while idx < n:\n",
    "        start_idx = idx\n",
    "        words = 0\n",
    "        end_idx = idx\n",
    "\n",
    "        while end_idx < n:\n",
    "            sent_len = len(sentences[end_idx].split())\n",
    "            if words + sent_len > max_words and words > 0:\n",
    "                break\n",
    "            words += sent_len\n",
    "            end_idx += 1\n",
    "\n",
    "        if end_idx == start_idx:\n",
    "            end_idx = min(start_idx + 1, n)\n",
    "            words = len(sentences[start_idx].split())\n",
    "\n",
    "        chunk_text = \" \".join(sentences[start_idx:end_idx]).strip()\n",
    "        chunks.append(chunk_text)\n",
    "        chunk_num += 1\n",
    "\n",
    "        if end_idx >= n:\n",
    "            break\n",
    "\n",
    "        overlap_count = 0\n",
    "        j = end_idx - 1\n",
    "\n",
    "        while j > start_idx and overlap_count < overlap_words:\n",
    "            overlap_count += len(sentences[j].split())\n",
    "            j -= 1\n",
    "\n",
    "        overlap_start_idx = j + 1\n",
    "\n",
    "        if overlap_start_idx <= start_idx:\n",
    "            idx = end_idx\n",
    "        else:\n",
    "            idx = overlap_start_idx\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ================== MAIN PROCESSING ==================\n",
    "\n",
    "def process_all_documents():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    docs = {\n",
    "        \"Graduate\":      \"Graduate.txt\",\n",
    "        \"SkilledWorker\": \"SkilledWorker.txt\",\n",
    "        \"HealthCare\":    \"HealthCare.txt\",\n",
    "        \"Student\":       \"Student.txt\",\n",
    "        \"Visitor\":       \"Visitor.txt\"\n",
    "    }\n",
    "\n",
    "    all_chunks = []   # STORE EVERYTHING HERE\n",
    "\n",
    "    for visa_type, fname in docs.items():\n",
    "        text_path = os.path.join(EXTRACTED_DIR, fname)\n",
    "\n",
    "        if not os.path.exists(text_path):\n",
    "            print(f\"‚ö† File not found: {text_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {visa_type} ‚Üí {text_path}\")\n",
    "\n",
    "        raw_text = read_text(text_path)\n",
    "        sentences = split_into_sentences(raw_text)\n",
    "\n",
    "        if not sentences:\n",
    "            print(f\"‚ö† No sentences found for {visa_type}\")\n",
    "            continue\n",
    "\n",
    "        chunks_text = build_chunks_from_sentences(sentences)\n",
    "\n",
    "        # Add metadata for each chunk\n",
    "        for idx, ch in enumerate(chunks_text, start=1):\n",
    "            all_chunks.append({\n",
    "                \"id\": f\"{visa_type}_{idx}\",\n",
    "                \"visa_type\": visa_type,\n",
    "                \"chunk_index\": idx,\n",
    "                \"source_file\": fname,\n",
    "                \"word_count\": len(ch.split()),\n",
    "                \"max_words\": MAX_WORDS,\n",
    "                \"overlap_words\": OVERLAP_WORDS,\n",
    "                \"text\": ch\n",
    "            })\n",
    "\n",
    "        print(f\"‚úî {visa_type}: {len(chunks_text)} chunks created.\")\n",
    "\n",
    "    # SAVE ONE UNIFIED JSON\n",
    "    out_path = os.path.join(OUTPUT_DIR, \"all_visa_chunks.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nüéâ ALL DONE! Unified chunk file saved ‚Üí {out_path}\")\n",
    "    print(f\"Total chunks: {len(all_chunks)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3aa6650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded unified chunk JSON.\n",
      "Visa types found: ['Graduate', 'SkilledWorker', 'HealthCare', 'Student', 'Visitor']\n",
      "\n",
      "\n",
      "Extracting PDF text for: Graduate\n",
      "\n",
      "===============================\n",
      "Comparing Graduate PDF vs Chunks\n",
      "===============================\n",
      "\n",
      "Similarity Score: 72.35%\n",
      "Coverage Score: 100.00%\n",
      "\n",
      "Perfect coverage! No words missing.\n",
      "\n",
      "Extracting PDF text for: SkilledWorker\n",
      "\n",
      "===============================\n",
      "Comparing SkilledWorker PDF vs Chunks\n",
      "===============================\n",
      "\n",
      "Similarity Score: 47.54%\n",
      "Coverage Score: 100.00%\n",
      "\n",
      "Perfect coverage! No words missing.\n",
      "\n",
      "Extracting PDF text for: HealthCare\n",
      "\n",
      "===============================\n",
      "Comparing HealthCare PDF vs Chunks\n",
      "===============================\n",
      "\n",
      "Similarity Score: 44.49%\n",
      "Coverage Score: 100.00%\n",
      "\n",
      "Perfect coverage! No words missing.\n",
      "\n",
      "Extracting PDF text for: Student\n",
      "\n",
      "===============================\n",
      "Comparing Student PDF vs Chunks\n",
      "===============================\n",
      "\n",
      "Similarity Score: 64.57%\n",
      "Coverage Score: 100.00%\n",
      "\n",
      "Perfect coverage! No words missing.\n",
      "\n",
      "Extracting PDF text for: Visitor\n",
      "\n",
      "===============================\n",
      "Comparing Visitor PDF vs Chunks\n",
      "===============================\n",
      "\n",
      "Similarity Score: 32.50%\n",
      "Coverage Score: 100.00%\n",
      "\n",
      "Perfect coverage! No words missing.\n",
      "\n",
      "\n",
      "===============================\n",
      "FINAL PDF vs CHUNK COMPARISON REPORT\n",
      "===============================\n",
      "\n",
      "Graduate:\n",
      "   Similarity: 72.35%\n",
      "   Coverage:   100.00%\n",
      "\n",
      "SkilledWorker:\n",
      "   Similarity: 47.54%\n",
      "   Coverage:   100.00%\n",
      "\n",
      "HealthCare:\n",
      "   Similarity: 44.49%\n",
      "   Coverage:   100.00%\n",
      "\n",
      "Student:\n",
      "   Similarity: 64.57%\n",
      "   Coverage:   100.00%\n",
      "\n",
      "Visitor:\n",
      "   Similarity: 32.50%\n",
      "   Coverage:   100.00%\n",
      "\n",
      "üéâ Comparison completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===============================\n",
    "# CONFIG PATHS\n",
    "# ===============================\n",
    "\n",
    "PDF_DIR = r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\code\\Dataset\\UK\"\n",
    "CHUNK_JSON = r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\\all_visa_chunks.json\"\n",
    "\n",
    "PDF_FILES = {\n",
    "    \"Graduate\":      \"Graduate.pdf\",\n",
    "    \"SkilledWorker\": \"SkilledWorker.pdf\",\n",
    "    \"HealthCare\":    \"HealthCare.pdf\",\n",
    "    \"Student\":       \"Student.pdf\",\n",
    "    \"Visitor\":       \"Visitor.pdf\",\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# PDF TEXT EXTRACTOR\n",
    "# ===============================\n",
    "\n",
    "def extract_pdf_text(path):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "\n",
    "    # Normalize spacing\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# LOAD CHUNK JSON\n",
    "# ===============================\n",
    "\n",
    "with open(CHUNK_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# Group chunks by visa_type\n",
    "chunk_map = defaultdict(str)\n",
    "for ch in chunks:\n",
    "    chunk_map[ch[\"visa_type\"]] += \" \" + ch[\"text\"]\n",
    "\n",
    "print(\"Loaded unified chunk JSON.\")\n",
    "print(f\"Visa types found: {list(chunk_map.keys())}\\n\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# COVERAGE COMPARISON FUNCTION\n",
    "# ===============================\n",
    "\n",
    "def text_similarity(a, b):\n",
    "    \"\"\"Return similarity ratio between two long strings.\"\"\"\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "def compare_pdf_and_chunks(visa_type, pdf_text, chunk_text):\n",
    "    print(f\"\\n===============================\")\n",
    "    print(f\"Comparing {visa_type} PDF vs Chunks\")\n",
    "    print(f\"===============================\\n\")\n",
    "\n",
    "    # Normalize both\n",
    "    pdf_clean = re.sub(r\"\\s+\", \" \", pdf_text).strip().lower()\n",
    "    chunk_clean = re.sub(r\"\\s+\", \" \", chunk_text).strip().lower()\n",
    "\n",
    "    # Compute similarity\n",
    "    sim = text_similarity(pdf_clean, chunk_clean)\n",
    "    print(f\"Similarity Score: {sim*100:.2f}%\")\n",
    "\n",
    "    # Coverage: how much PDF text appears in chunks\n",
    "    pdf_words = pdf_clean.split()\n",
    "    missing_words = [w for w in pdf_words if w not in chunk_clean]\n",
    "\n",
    "    miss_ratio = len(missing_words) / len(pdf_words)\n",
    "    coverage = 1 - miss_ratio\n",
    "\n",
    "    print(f\"Coverage Score: {coverage*100:.2f}%\")\n",
    "\n",
    "    # Show example missing words\n",
    "    if len(missing_words) > 0:\n",
    "        print(\"\\nExample missing words (sample of 20):\")\n",
    "        print(missing_words[:20])\n",
    "    else:\n",
    "        print(\"\\nPerfect coverage! No words missing.\")\n",
    "\n",
    "    return sim, coverage\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# RUN COMPARISON FOR ALL PDFS\n",
    "# ===============================\n",
    "\n",
    "results = {}\n",
    "\n",
    "for visa_type, pdf_name in PDF_FILES.items():\n",
    "    print(f\"\\nExtracting PDF text for: {visa_type}\")\n",
    "\n",
    "    pdf_path = f\"{PDF_DIR}\\\\{pdf_name}\"\n",
    "    pdf_text = extract_pdf_text(pdf_path)\n",
    "\n",
    "    chunk_text = chunk_map[visa_type]\n",
    "\n",
    "    sim, coverage = compare_pdf_and_chunks(visa_type, pdf_text, chunk_text)\n",
    "\n",
    "    results[visa_type] = {\n",
    "        \"similarity\": sim,\n",
    "        \"coverage\": coverage\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# FINAL REPORT\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\\n===============================\")\n",
    "print(\"FINAL PDF vs CHUNK COMPARISON REPORT\")\n",
    "print(\"===============================\\n\")\n",
    "\n",
    "for visa, vals in results.items():\n",
    "    print(f\"{visa}:\")\n",
    "    print(f\"   Similarity: {vals['similarity']*100:.2f}%\")\n",
    "    print(f\"   Coverage:   {vals['coverage']*100:.2f}%\\n\")\n",
    "\n",
    "print(\"üéâ Comparison completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56978730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 36 chunks.\n",
      "\n",
      "üìå Basic Chunk Statistics\n",
      "---------------------------\n",
      "Total Chunks: 36\n",
      "Average Chunk Size: 223.42 words\n",
      "Minimum Chunk Size: 148 words\n",
      "Maximum Chunk Size: 250 words\n",
      "Std Deviation: 27.31 words\n",
      "\n",
      "üìå Chunk Size Consistency\n",
      "---------------------------\n",
      "Balance Score (200‚Äì300 words): 86.11% chunks well sized\n",
      "\n",
      "üìå Semantic Integrity\n",
      "---------------------------\n",
      "Starts at a natural boundary: 31/36\n",
      "Ends at a natural sentence:   25/36\n",
      "Semantic Integrity Score:     77.78%\n",
      "\n",
      "üìå Overlap Continuity\n",
      "---------------------------\n",
      "Average Overlap Ratio: 20.00%\n",
      "\n",
      "üìå Visa-wise Chunk Count\n",
      "---------------------------\n",
      "Graduate: 7 chunks\n",
      "SkilledWorker: 7 chunks\n",
      "HealthCare: 7 chunks\n",
      "Student: 6 chunks\n",
      "Visitor: 9 chunks\n",
      "\n",
      "üìå Approximate Coverage Score\n",
      "---------------------------\n",
      "Coverage (chunks > 100 words): 100.00%\n",
      "\n",
      "üìå Anomaly Check\n",
      "---------------------------\n",
      "No major anomalies detected üëç\n",
      "\n",
      "üéâ Chunk Evaluation Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# =======================\n",
    "# LOAD YOUR UNIFIED JSON\n",
    "# =======================\n",
    "PATH = r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\\all_visa_chunks.json\"\n",
    "\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks.\\n\")\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 1. BASIC CHUNK STATS\n",
    "# =======================\n",
    "word_counts = [c[\"word_count\"] for c in chunks]\n",
    "\n",
    "print(\"üìå Basic Chunk Statistics\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n",
    "print(f\"Average Chunk Size: {np.mean(word_counts):.2f} words\")\n",
    "print(f\"Minimum Chunk Size: {np.min(word_counts)} words\")\n",
    "print(f\"Maximum Chunk Size: {np.max(word_counts)} words\")\n",
    "print(f\"Std Deviation: {np.std(word_counts):.2f} words\\n\")\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 2. CHUNK SIZE CONSISTENCY SCORE\n",
    "# =======================\n",
    "def chunk_balance_score(wc):\n",
    "    ideal_min, ideal_max = 200, 300\n",
    "    ok = sum(1 for x in wc if ideal_min <= x <= ideal_max)\n",
    "    return ok / len(wc)\n",
    "\n",
    "balance_score = chunk_balance_score(word_counts)\n",
    "\n",
    "print(\"üìå Chunk Size Consistency\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Balance Score (200‚Äì300 words): {balance_score*100:.2f}% chunks well sized\\n\")\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 3. SEMANTIC INTEGRITY SCORE\n",
    "# =======================\n",
    "def starts_well(text):\n",
    "    # good if starts with capital letter, not mid-sentence\n",
    "    return bool(re.match(r\"^[A-Z0-9]\", text.strip()))\n",
    "\n",
    "def ends_well(text):\n",
    "    return text.strip().endswith(('.', '?', '!'))\n",
    "\n",
    "start_ok = sum(starts_well(c[\"text\"]) for c in chunks)\n",
    "end_ok = sum(ends_well(c[\"text\"]) for c in chunks)\n",
    "\n",
    "semantic_integrity = ((start_ok + end_ok) / (2 * len(chunks)))\n",
    "\n",
    "print(\"üìå Semantic Integrity\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Starts at a natural boundary: {start_ok}/{len(chunks)}\")\n",
    "print(f\"Ends at a natural sentence:   {end_ok}/{len(chunks)}\")\n",
    "print(f\"Semantic Integrity Score:     {semantic_integrity*100:.2f}%\\n\")\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 4. OVERLAP CONTINUITY SCORE\n",
    "# =======================\n",
    "def overlap_ratio(prev, curr):\n",
    "    prev_words = prev.split()[-40:]   # last 40 words of previous\n",
    "    curr_words = curr.split()[:40]    # first 40 words of current\n",
    "    intersect = len(set(prev_words) & set(curr_words))\n",
    "    return intersect / 40\n",
    "\n",
    "visa_groups = defaultdict(list)\n",
    "for c in chunks:\n",
    "    visa_groups[c[\"visa_type\"]].append(c)\n",
    "\n",
    "overlap_scores = []\n",
    "for visa_type, group in visa_groups.items():\n",
    "    group = sorted(group, key=lambda x: x[\"chunk_index\"])\n",
    "    for i in range(1, len(group)):\n",
    "        score = overlap_ratio(group[i-1][\"text\"], group[i][\"text\"])\n",
    "        overlap_scores.append(score)\n",
    "\n",
    "print(\"üìå Overlap Continuity\")\n",
    "print(\"---------------------------\")\n",
    "if overlap_scores:\n",
    "    print(f\"Average Overlap Ratio: {np.mean(overlap_scores)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"Not enough chunks to compute overlap.\")\n",
    "print()\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 5. VISA-WISE CHUNK DISTRIBUTION\n",
    "# =======================\n",
    "visa_count = Counter([c[\"visa_type\"] for c in chunks])\n",
    "\n",
    "print(\"üìå Visa-wise Chunk Count\")\n",
    "print(\"---------------------------\")\n",
    "for visa, count in visa_count.items():\n",
    "    print(f\"{visa}: {count} chunks\")\n",
    "print()\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 6. COVERAGE SCORE (APPROX)\n",
    "# =======================\n",
    "# Coverage is approximated as: no chunk extremely small + no missing sections\n",
    "coverage_good = sum(1 for w in word_counts if w > 100)\n",
    "coverage_score = coverage_good / len(word_counts)\n",
    "\n",
    "print(\"üìå Approximate Coverage Score\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Coverage (chunks > 100 words): {coverage_score*100:.2f}%\\n\")\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 7. ANOMALY DETECTION\n",
    "# =======================\n",
    "print(\"üìå Anomaly Check\")\n",
    "print(\"---------------------------\")\n",
    "for visa, count in visa_count.items():\n",
    "    if count < 3:\n",
    "        print(f\"‚ö† Warning: {visa} has very few chunks ‚Üí extraction may be incomplete.\")\n",
    "if np.min(word_counts) < 80:\n",
    "    print(\"‚ö† Warning: Some chunks have unusually low word count (<80).\")\n",
    "if balance_score < 0.70:\n",
    "    print(\"‚ö† Warning: Many chunks fall outside ideal size range.\")\n",
    "else:\n",
    "    print(\"No major anomalies detected üëç\")\n",
    "\n",
    "print(\"\\nüéâ Chunk Evaluation Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b569c918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kriti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\kriti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\kriti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kriti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kriti\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\kriti\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "\n",
      "Loaded unified chunk JSON.\n",
      "\n",
      "Evaluating Graduate...\n",
      "Evaluating SkilledWorker...\n",
      "Evaluating HealthCare...\n",
      "Evaluating Student...\n",
      "Evaluating Visitor...\n",
      "\n",
      "==============================\n",
      "SEMANTIC PDF vs CHUNK REPORT\n",
      "==============================\n",
      "\n",
      "Graduate:\n",
      "  Semantic Similarity: 88.09%\n",
      "  Semantic Coverage:   100.00%\n",
      "\n",
      "SkilledWorker:\n",
      "  Semantic Similarity: 87.85%\n",
      "  Semantic Coverage:   100.00%\n",
      "\n",
      "HealthCare:\n",
      "  Semantic Similarity: 87.87%\n",
      "  Semantic Coverage:   100.00%\n",
      "\n",
      "Student:\n",
      "  Semantic Similarity: 85.09%\n",
      "  Semantic Coverage:   100.00%\n",
      "\n",
      "Visitor:\n",
      "  Semantic Similarity: 85.10%\n",
      "  Semantic Coverage:   100.00%\n",
      "\n",
      "‚úÖ Semantic evaluation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "PDF_DIR = r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\code\\Dataset\\UK\"\n",
    "CHUNK_JSON = r\"C:\\Users\\kriti\\OneDrive\\Desktop\\Infosys\\chunks\\all_visa_chunks.json\"\n",
    "\n",
    "PDF_FILES = {\n",
    "    \"Graduate\": \"Graduate.pdf\",\n",
    "    \"SkilledWorker\": \"SkilledWorker.pdf\",\n",
    "    \"HealthCare\": \"HealthCare.pdf\",\n",
    "    \"Student\": \"Student.pdf\",\n",
    "    \"Visitor\": \"Visitor.pdf\"\n",
    "}\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "SIM_THRESHOLD = 0.70   # semantic match threshold\n",
    "\n",
    "# =========================\n",
    "# LOAD EMBEDDING MODEL\n",
    "# =========================\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "print(\"Model loaded.\\n\")\n",
    "\n",
    "# =========================\n",
    "# PDF TEXT EXTRACTION\n",
    "# =========================\n",
    "\n",
    "def extract_pdf_text(path):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def split_pdf_semantically(text, max_words=120):\n",
    "    words = text.split()\n",
    "    segments = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        segments.append(\" \".join(words[start:start+max_words]))\n",
    "        start += max_words\n",
    "    return segments\n",
    "\n",
    "# =========================\n",
    "# LOAD CHUNKS\n",
    "# =========================\n",
    "\n",
    "with open(CHUNK_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "chunk_map = defaultdict(list)\n",
    "for ch in chunks:\n",
    "    chunk_map[ch[\"visa_type\"]].append(ch[\"text\"])\n",
    "\n",
    "print(\"Loaded unified chunk JSON.\\n\")\n",
    "\n",
    "# =========================\n",
    "# SEMANTIC COMPARISON\n",
    "# =========================\n",
    "\n",
    "def semantic_compare(pdf_segments, chunk_texts):\n",
    "    pdf_emb = model.encode(pdf_segments, normalize_embeddings=True)\n",
    "    chunk_emb = model.encode(chunk_texts, normalize_embeddings=True)\n",
    "\n",
    "    sim_matrix = cosine_similarity(pdf_emb, chunk_emb)\n",
    "\n",
    "    max_sims = sim_matrix.max(axis=1)\n",
    "\n",
    "    avg_similarity = np.mean(max_sims)\n",
    "    coverage = np.mean(max_sims >= SIM_THRESHOLD)\n",
    "\n",
    "    return avg_similarity, coverage\n",
    "\n",
    "# =========================\n",
    "# RUN EVALUATION\n",
    "# =========================\n",
    "\n",
    "results = {}\n",
    "\n",
    "for visa, pdf_name in PDF_FILES.items():\n",
    "    print(f\"Evaluating {visa}...\")\n",
    "\n",
    "    pdf_path = f\"{PDF_DIR}\\\\{pdf_name}\"\n",
    "    pdf_text = extract_pdf_text(pdf_path)\n",
    "\n",
    "    pdf_segments = split_pdf_semantically(pdf_text)\n",
    "    chunk_texts = chunk_map[visa]\n",
    "\n",
    "    avg_sim, coverage = semantic_compare(pdf_segments, chunk_texts)\n",
    "\n",
    "    results[visa] = {\n",
    "        \"avg_similarity\": avg_sim,\n",
    "        \"semantic_coverage\": coverage\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# FINAL REPORT\n",
    "# =========================\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"SEMANTIC PDF vs CHUNK REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "for visa, vals in results.items():\n",
    "    print(f\"{visa}:\")\n",
    "    print(f\"  Semantic Similarity: {vals['avg_similarity']*100:.2f}%\")\n",
    "    print(f\"  Semantic Coverage:   {vals['semantic_coverage']*100:.2f}%\\n\")\n",
    "\n",
    "print(\"‚úÖ Semantic evaluation completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
